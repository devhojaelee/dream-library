#!/usr/bin/env python3
"""
ì±… ë©”íƒ€ë°ì´í„° ë³´ì™„ ìŠ¤í¬ë¦½íŠ¸

Z-Libraryì—ì„œ ê°€ì ¸ì˜¤ì§€ ëª»í•œ ì±… í‘œì§€ì™€ ì„¤ëª…ì„ Naver Books APIë¥¼ í†µí•´ ë³´ì™„í•©ë‹ˆë‹¤.
"""

import os
import json
import requests
import time
import re
from pathlib import Path
from typing import Optional, Dict, Tuple
from io import BytesIO
from PIL import Image
from difflib import SequenceMatcher
from datetime import datetime
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì½ìŒ)
env_path = 'web/.env'
if os.path.exists(env_path):
    load_dotenv(env_path)
else:
    load_dotenv()  # ê¸°ë³¸ .env íŒŒì¼ ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜

NAVER_CLIENT_ID = os.getenv('NAVER_CLIENT_ID')
NAVER_CLIENT_SECRET = os.getenv('NAVER_CLIENT_SECRET')

# ê²½ë¡œ ì„¤ì •
BOOKS_DIR = Path("books")
METADATA_DIR = BOOKS_DIR / "metadata"
COVERS_DIR = BOOKS_DIR / "covers"

# Naver Books API ì„¤ì •
NAVER_BOOKS_API = "https://openapi.naver.com/v1/search/book.json"

# í•´ìƒë„ ì„ê³„ê°’ (ì´ë³´ë‹¤ ì‘ìœ¼ë©´ ìŠ¤í‚µ)
MIN_COVER_WIDTH = 200

# ì œëª© ìœ ì‚¬ë„ ì„ê³„ê°’ (ì´ë³´ë‹¤ ë‚®ìœ¼ë©´ ë‹¤ë¥¸ ì±…ìœ¼ë¡œ íŒë‹¨)
MIN_TITLE_SIMILARITY = 0.6

class MetadataEnricher:
    def __init__(self):
        self.updated_count = 0
        self.failed_count = 0
        self.skipped_count = 0

        # API í‚¤ ê²€ì¦
        if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
            raise ValueError("Naver API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. web/.env íŒŒì¼ì— NAVER_CLIENT_IDì™€ NAVER_CLIENT_SECRETë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")

    def clean_title(self, title: str) -> str:
        """ì œëª© ì •ì œ: ê´„í˜¸, ëŒ€ê´„í˜¸ ë‚´ìš© ì œê±°"""
        # (ê°œì •íŒ) ì œê±°
        cleaned = re.sub(r'\([^)]*\)', '', title)

        # [íœ´ê³ ìƒ ìˆ˜ìƒì‘] ì œê±°
        cleaned = re.sub(r'\[[^\]]*\]', '', cleaned)

        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        cleaned = re.sub(r'\s+', ' ', cleaned)

        return cleaned.strip()

    def clean_html_tags(self, text: str) -> str:
        """HTML íƒœê·¸ ë° ì—”í‹°í‹° ì œê±°"""
        if not text:
            return ""

        # HTML íƒœê·¸ ì œê±°
        cleaned = re.sub(r'<[^>]*>', '', text)

        # HTML ì—”í‹°í‹° ë³€í™˜
        cleaned = cleaned.replace('&lt;', '<')
        cleaned = cleaned.replace('&gt;', '>')
        cleaned = cleaned.replace('&amp;', '&')
        cleaned = cleaned.replace('&quot;', '"')
        cleaned = cleaned.replace('&#39;', "'")

        # ì—°ì†ëœ ì¤„ë°”ê¿ˆì„ 2ê°œë¡œ ì •ê·œí™” (ë¬¸ë‹¨ êµ¬ë¶„)
        cleaned = re.sub(r'\n{3,}', '\n\n', cleaned)

        return cleaned.strip()

    def clean_title_for_comparison(self, title: str) -> str:
        """ë¹„êµìš© ì œëª© ì •ì œ: ê´„í˜¸/ëŒ€ê´„í˜¸ ì´ì „ ë¶€ë¶„ë§Œ ì¶”ì¶œ"""
        # ê´„í˜¸ ì´ì „ê¹Œì§€ë§Œ ì¶”ì¶œ
        match = re.match(r'^([^\(\[]+)', title)
        if match:
            cleaned = match.group(1).strip()
        else:
            cleaned = title

        # ì†Œë¬¸ì ë³€í™˜ ë° ê³µë°± ì •ê·œí™”
        cleaned = re.sub(r'\s+', ' ', cleaned.lower().strip())
        return cleaned

    def calculate_title_similarity(self, title1: str, title2: str) -> float:
        """ë‘ ì œëª©ì˜ ìœ ì‚¬ë„ ê³„ì‚° (0.0 ~ 1.0)"""
        # ê´„í˜¸ ì´ì „ ë¶€ë¶„ë§Œ ë¹„êµ
        t1 = self.clean_title_for_comparison(title1)
        t2 = self.clean_title_for_comparison(title2)

        # SequenceMatcherë¥¼ ì‚¬ìš©í•œ ìœ ì‚¬ë„ ê³„ì‚°
        return SequenceMatcher(None, t1, t2).ratio()

    def search_naver_books_api_single(self, query: str, original_title: str) -> Optional[Dict]:
        """Naver Books APIë¡œ ë‹¨ì¼ ì¿¼ë¦¬ ê²€ìƒ‰ (ì œëª© ìœ ì‚¬ë„ ê²€ì¦ í¬í•¨)"""
        try:
            headers = {
                'X-Naver-Client-Id': NAVER_CLIENT_ID,
                'X-Naver-Client-Secret': NAVER_CLIENT_SECRET,
            }

            params = {
                'query': query,
                'display': 10,  # ìµœëŒ€ 10ê°œ ê²°ê³¼
            }

            response = requests.get(NAVER_BOOKS_API, headers=headers, params=params, timeout=10)

            if response.status_code == 200:
                data = response.json()

                if 'items' in data and len(data['items']) > 0:
                    best_match = None
                    best_score = 0

                    for item in data['items']:
                        # HTML íƒœê·¸ ì œê±°
                        clean_title = self.clean_html_tags(item.get('title', ''))
                        clean_description = self.clean_html_tags(item.get('description', ''))
                        clean_author = item.get('author', '').replace('^', ', ')

                        # ì œëª© ìœ ì‚¬ë„ ê²€ì¦
                        similarity = self.calculate_title_similarity(original_title, clean_title)

                        # ìœ ì‚¬ë„ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ ìŠ¤í‚µ
                        if similarity < MIN_TITLE_SIMILARITY:
                            continue

                        # ì™„ì „ì„± ì ìˆ˜ ê³„ì‚°
                        completeness_score = 0
                        if clean_description:
                            completeness_score += 2
                        if item.get('image'):
                            completeness_score += 2
                        if clean_author:
                            completeness_score += 1
                        if item.get('pubdate'):
                            completeness_score += 1

                        # ì¢…í•© ì ìˆ˜ = ì™„ì „ì„± + ìœ ì‚¬ë„ ë³´ë„ˆìŠ¤
                        total_score = completeness_score + (similarity * 3)

                        if total_score > best_score:
                            best_score = total_score
                            # ê²°ê³¼ ì •ì œí•´ì„œ ì €ì¥
                            best_match = {
                                'title': clean_title,
                                'author': clean_author,
                                'publisher': item.get('publisher', ''),
                                'pubdate': item.get('pubdate', ''),
                                'description': clean_description,
                                'image': item.get('image', ''),
                                'similarity': similarity
                            }

                    return best_match

            return None

        except Exception as e:
            print(f"  âš ï¸  Naver Books API ì˜¤ë¥˜: {e}")
            return None

    def search_naver_books_api(self, title: str, author: str = None) -> Optional[Dict]:
        """ì ì§„ì  ê²€ìƒ‰ ì „ëµìœ¼ë¡œ ì±… ì •ë³´ ê²€ìƒ‰"""
        print(f"  ğŸ” ê²€ìƒ‰ ì „ëµ ì‹œì‘...")

        # ì •ì œëœ ì œëª© ì¤€ë¹„
        cleaned_title = self.clean_title(title)

        # ê²€ìƒ‰ ì‹œë„ ìˆœì„œ
        search_attempts = []

        # 1ì°¨: ì›ë³¸ ì œëª© + ì €ì
        if author:
            query = f"{title} {author}"
            search_attempts.append(("ì›ë³¸+ì €ì", query))

        # 2ì°¨: ì •ì œëœ ì œëª© + ì €ì (ì›ë³¸ê³¼ ë‹¤ë¥¼ ë•Œë§Œ)
        if author and cleaned_title != title:
            query = f"{cleaned_title} {author}"
            search_attempts.append(("ì •ì œ+ì €ì", query))

        # 3ì°¨: ì •ì œëœ ì œëª©ë§Œ
        if cleaned_title != title:
            search_attempts.append(("ì •ì œ", cleaned_title))

        # 4ì°¨: ì›ë³¸ ì œëª©ë§Œ
        search_attempts.append(("ì›ë³¸", title))

        # ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„
        for attempt_name, query in search_attempts:
            print(f"  ğŸ“– [{attempt_name}] '{query}' ê²€ìƒ‰ ì¤‘...")
            result = self.search_naver_books_api_single(query, title)

            if result:
                result_title = result.get('title', 'ì•Œ ìˆ˜ ì—†ìŒ')
                similarity = result.get('similarity', 0)
                print(f"  âœ… ë°œê²¬! '{result_title}' (ìœ ì‚¬ë„: {similarity:.2f})")
                return result
            else:
                print(f"  âŒ ê²°ê³¼ ì—†ìŒ")

        return None

    def download_cover_image(self, image_url: str, filename: str) -> Optional[str]:
        """í‘œì§€ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
        try:
            # HTTPë¥¼ HTTPSë¡œ ë³€ê²½
            if image_url.startswith('http://'):
                image_url = image_url.replace('http://', 'https://')

            response = requests.get(image_url, timeout=10)

            if response.status_code == 200 and len(response.content) > 1000:
                # ì´ë¯¸ì§€ í˜•ì‹ ê°ì§€ (magic number í™•ì¸)
                if response.content[:8].startswith(b'\x89PNG'):
                    ext = '.png'
                elif response.content[:3].startswith(b'\xff\xd8\xff'):
                    ext = '.jpg'
                else:
                    ext = '.jpg'  # ê¸°ë³¸ê°’

                cover_filename = f"{filename.replace('.epub', '')}{ext}"
                cover_path = COVERS_DIR / cover_filename

                with open(cover_path, 'wb') as f:
                    f.write(response.content)

                size_kb = len(response.content) // 1024
                print(f"  ğŸ“ ì´ë¯¸ì§€ í¬ê¸°: {size_kb}KB ({ext[1:].upper()})")
                return cover_filename
            else:
                print(f"  âš ï¸  ìœ íš¨í•œ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return None

        except Exception as e:
            print(f"  âš ï¸  í‘œì§€ ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
            return None

    def enrich_metadata(self, epub_filename: str) -> bool:
        """ë‹¨ì¼ ì±…ì˜ ë©”íƒ€ë°ì´í„° ë³´ì™„"""
        title = epub_filename.replace('.epub', '')
        metadata_path = METADATA_DIR / f"{title}.json"

        # ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ì½ê¸°
        metadata = {}
        if metadata_path.exists():
            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)

        # ì´ë¯¸ ì‹œë„í–ˆìœ¼ë©´ ìŠ¤í‚µ
        if metadata.get('enrichment_attempted'):
            print(f"â­ï¸  {title[:50]}... - ì´ë¯¸ ë³´ì™„ ì‹œë„í•¨")
            self.skipped_count += 1
            return False

        # ì´ë¯¸ ì™„ì „í•œ ë©”íƒ€ë°ì´í„°ê°€ ìˆìœ¼ë©´ ìŠ¤í‚µ (ì‹¤ì œ íŒŒì¼ ì¡´ì¬ í™•ì¸)
        has_description = metadata.get('description') is not None

        cover_filename = metadata.get('cover')
        has_cover = False
        if cover_filename:
            cover_path = COVERS_DIR / cover_filename
            has_cover = cover_path.exists()  # ì‹¤ì œ íŒŒì¼ ì¡´ì¬ í™•ì¸

        if has_cover and has_description:
            print(f"â­ï¸  {title[:50]}... - ì´ë¯¸ ì™„ì „í•œ ë©”íƒ€ë°ì´í„° ì¡´ì¬")
            metadata['enrichment_attempted'] = True
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            self.skipped_count += 1
            return False

        print(f"ğŸ” {title[:50]}...")

        # ì €ì ì •ë³´ ì¶”ì¶œ (ìˆìœ¼ë©´ ê²€ìƒ‰ì— í™œìš©)
        author = metadata.get('author')

        # Naver Books APIì—ì„œ ê²€ìƒ‰
        book_info = self.search_naver_books_api(title, author)

        if not book_info:
            print(f"  âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            # ì‹¤íŒ¨í•´ë„ í”Œë˜ê·¸ ì €ì¥ (ì¬ì‹œë„ ë°©ì§€)
            metadata['enrichment_attempted'] = True
            METADATA_DIR.mkdir(parents=True, exist_ok=True)
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            self.failed_count += 1
            return False

        updated = False

        # ì„¤ëª… ë³´ì™„
        if not has_description and book_info.get('description'):
            description = book_info['description']
            metadata['description'] = description
            print(f"  âœ… ì„¤ëª… ì¶”ê°€: {description[:50]}...")
            updated = True

        # í‘œì§€ ë³´ì™„
        if not has_cover and book_info.get('image'):
            image_url = book_info['image']

            cover_filename = self.download_cover_image(image_url, epub_filename)

            if cover_filename:
                metadata['cover'] = cover_filename
                # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€ (ìºì‹œ ë¬´íš¨í™”ìš©)
                metadata['cover_updated'] = str(int(time.time() * 1000))
                print(f"  âœ… í‘œì§€ ë‹¤ìš´ë¡œë“œ: {cover_filename}")
                updated = True

        # ê¸°íƒ€ ë©”íƒ€ë°ì´í„° ë³´ì™„
        if book_info.get('author') and not metadata.get('author'):
            metadata['author'] = book_info['author']
            print(f"  âœ… ì €ì ì¶”ê°€: {metadata['author']}")
            updated = True

        if book_info.get('pubdate') and not metadata.get('year'):
            # YYYYMMDD í˜•ì‹ì—ì„œ ì—°ë„ë§Œ ì¶”ì¶œ
            year = book_info['pubdate'][:4] if len(book_info['pubdate']) >= 4 else book_info['pubdate']
            metadata['year'] = year
            print(f"  âœ… ì¶œíŒë…„ë„ ì¶”ê°€: {year}")
            updated = True

        # ë©”íƒ€ë°ì´í„° ì €ì¥
        # ì‹œë„í–ˆìŒì„ í‘œì‹œ (ì„±ê³µ/ì‹¤íŒ¨ ë¬´ê´€)
        metadata['enrichment_attempted'] = True

        if updated:
            METADATA_DIR.mkdir(parents=True, exist_ok=True)

            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            self.updated_count += 1
            print(f"  ğŸ’¾ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ")
            return True
        else:
            print(f"  âš ï¸  ë³´ì™„í•  ì •ë³´ ì—†ìŒ")
            # ì—…ë°ì´íŠ¸ ì—†ì–´ë„ í”Œë˜ê·¸ëŠ” ì €ì¥
            METADATA_DIR.mkdir(parents=True, exist_ok=True)
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            self.failed_count += 1
            return False

    def run(self):
        """ëª¨ë“  EPUB íŒŒì¼ì˜ ë©”íƒ€ë°ì´í„° ë³´ì™„"""
        start_time = datetime.now()
        print("\n" + "=" * 60)
        print("ğŸ“š Dream Library ë©”íƒ€ë°ì´í„° ë³´ì™„ ì‹œì‘")
        print(f"ğŸ• ì‹¤í–‰ ì‹œê°„: {start_time.strftime('%Y-%m-%d %H:%M:%S %Z')}")
        print("=" * 60)

        # covers ë””ë ‰í† ë¦¬ ìƒì„±
        COVERS_DIR.mkdir(parents=True, exist_ok=True)

        # Check for download_status.json to get list of newly downloaded files
        status_path = BOOKS_DIR / "download_status.json"
        target_titles = None

        if status_path.exists():
            try:
                with open(status_path, 'r', encoding='utf-8') as f:
                    status_data = json.load(f)
                    if 'downloadedFiles' in status_data:
                        # Empty list is valid - means 0 new downloads
                        target_titles = set(status_data['downloadedFiles'])
                        print(f"ğŸ“‹ Processing {len(target_titles)} newly downloaded files from download_status.json")
                        print("=" * 60)
            except Exception as e:
                print(f"âš ï¸  Could not read download_status.json: {e}")
                print("  â†’ Processing all files instead")

        # EPUB íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        all_epub_files = list(BOOKS_DIR.glob("*.epub"))

        if not all_epub_files:
            print("âŒ EPUB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        # Filter to only newly downloaded files if we have the list
        if target_titles is not None:
            # We have download_status.json with downloadedFiles
            if len(target_titles) == 0:
                # Empty list = no new downloads, skip processing
                print("â­ï¸  No new files to process (downloadedFiles is empty), skipping enrichment")
                return

            epub_files = [f for f in all_epub_files if f.stem in target_titles]
            print(f"ğŸ¯ Found {len(epub_files)} matching files out of {len(all_epub_files)} total")
            if len(epub_files) == 0:
                print("âš ï¸  No matching files found - all may have been already processed")
                return
        else:
            # No download_status.json, process all files (fallback)
            epub_files = all_epub_files
            print(f"ğŸ“š Processing all {len(epub_files)} files (no download list found)")

        print(f"\nì´ {len(epub_files)}ê°œì˜ ì±…ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n")

        for i, epub_path in enumerate(epub_files, 1):
            print(f"\n[{i}/{len(epub_files)}] ", end="")
            self.enrich_metadata(epub_path.name)

            # API í˜¸ì¶œ ì œí•œ ê³ ë ¤ (1ì´ˆ ëŒ€ê¸°)
            if i < len(epub_files):
                time.sleep(1)

        # ê²°ê³¼ ìš”ì•½
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        print("\n" + "=" * 60)
        print("ğŸ“Š ì²˜ë¦¬ ê²°ê³¼")
        print("=" * 60)
        print(f"ğŸ• ì‹œì‘ ì‹œê°„: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ• ì¢…ë£Œ ì‹œê°„: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"â±ï¸  ì†Œìš” ì‹œê°„: {duration:.1f}ì´ˆ")
        print(f"âœ… ì—…ë°ì´íŠ¸ë¨: {self.updated_count}ê°œ")
        print(f"â­ï¸  ìŠ¤í‚µë¨: {self.skipped_count}ê°œ")
        print(f"âŒ ì‹¤íŒ¨: {self.failed_count}ê°œ")
        print("=" * 60)

if __name__ == "__main__":
    enricher = MetadataEnricher()
    enricher.run()
